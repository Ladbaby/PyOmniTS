activation: gelu
augmentation_ratio: 0
batch_size: 128
c_out: 128
channel_independence: 1
checkpoints: storage/results/
checkpoints_test: null
collate_fn: collate_fn
cru_bandwidth: 3
cru_num_basis: 15
cru_ts: 1.0
d_ff: 2048
d_layers: 1
d_model: 512
d_timesteps: 1
dataset_file_name: openmic-2018.npz
dataset_name: OpenMIC
dataset_root_path: storage/datasets/OpenMIC
dec_in: 128
dropout: 0.05
e_layers: 3
embed: timeF
embed_type: 0
enc_in: 128
f: '1'
factor: 3
features: M
freq: h
gpu_id: 0
gpu_ids: null
hidden_layers: 1
individual: 0
informer_distil: 1
is_training: 1
itr: 1
kernel_size: 25
label_len: 0
latent_ode_classif: 0
latent_ode_gen_layers: 1
latent_ode_gru_units: 100
latent_ode_linear_classif: 0
latent_ode_rec_dims: 20
latent_ode_rec_layers: 1
latent_ode_units: 100
latent_ode_z0_encoder: odernn
learning_rate: 0.0001
load_checkpoints_test: 1
loss: CrossEntropyLoss
lr_scheduler: DelayedStepDecayLR
mamba_d_conv: 4
mamba_expand: 2
missing_rate: 0.0
model_id: PatchTST_10_0
model_name: PatchTST
moving_avg: 25
mtan_alpha: 100.0
mtan_num_ref_points: 8
n_classes: 20
n_heads: 16
n_layers: 1
n_train_stages: 1
neuralflows_flow_layers: 1
neuralflows_flow_model: coupling
neuralflows_latents: 20
neuralflows_time_hidden_dim: 1
neuralflows_time_net: TimeLinear
nonstationarytransformer_p_hidden_dims:
- 128
- 128
nonstationarytransformer_p_hidden_layers: 2
num_workers: 10
output_attention: 0
patch_len: 5
patch_len_max_irr: null
patch_stride: 5
patchtst_decomposition: 0
patchtst_fc_dropout: 0.05
patchtst_head_dropout: 0.0
patchtst_padding_patch: end
patchtst_subtract_last: 0
patience: 10
pred_len: 0
pred_len_max_irr: null
pretrained_checkpoint_file_name: ''
pretrained_checkpoint_root_path: ''
primenet_pooling: ave
retain_graph: 0
revin: 1
revin_affine: 0
save_arrays: 0
scale_factor: 2
seq_len: 10
seq_len_max_irr: null
sweep: 0
target_variable_index: 0
target_variable_name: OT
task_name: classification
test_all: 0
test_dataset_statistics: 0
test_flop: 0
test_gpu_memory: 0
test_train_time: 0
test_zero_shot: 0
timemixer_decomp_method: moving_avg
timemixer_down_sampling_layers: 0
timemixer_down_sampling_method: avg
timemixer_use_norm: 1
top_k: 5
train_epochs: 300
use_gpu: 1
use_multi_gpu: 0
val_interval: 1
wandb: 0
